{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coleta.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "18Z63eZLtt5i"
      },
      "source": [
        "# IMPORTACOES DE MODULOS E BIBLIOTECAS\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "from urllib.request import urlopen\r\n",
        "from urllib.error import HTTPError\r\n",
        "import csv"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35e0mZoVCi0s"
      },
      "source": [
        "# FUNCOES\r\n",
        "\r\n",
        "def getPage(url):\r\n",
        "  try:\r\n",
        "    resposta = urlopen(url)\r\n",
        "  except HTTPError as e:\r\n",
        "    return None\r\n",
        "  return resposta"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwqF--LrPPp5"
      },
      "source": [
        "links= []\r\n",
        "counts=[]\r\n",
        "counttemp = []\r\n",
        "datas= []\r\n",
        "dicionario = {}\r\n",
        "link_base =\" http://www.nuforc.org/webreports/ndxevent.html \"\r\n",
        "link_suporte = \"http://www.nuforc.org/webreports/\"\r\n",
        "resposta_http = getPage(link_base)\r\n",
        "objeto_soup = BeautifulSoup(resposta_http.read(), features=\"html.parser\")\r\n",
        "\r\n",
        "#Pegando as DATAS, LINKS\r\n",
        "for link in objeto_soup.find_all('a')[1:]:\r\n",
        "  if link.get_text()=='12/1996':\r\n",
        "    break\r\n",
        "  elif int(link.get_text()[3:])>2017:\r\n",
        "    continue\r\n",
        "  datas.append((link.get_text()))\r\n",
        "  links.append((link.get('href')))\r\n",
        "\r\n",
        "datas = datas[4:-8]\r\n",
        "links = links[4:-8]\r\n",
        "\r\n",
        "#Pegando os COUNTS\r\n",
        "for link in objeto_soup.find_all('td'):\r\n",
        "  counttemp.append((link.get_text()))\r\n",
        "counttemp = {counttemp[i]: counttemp[i+1] for i in range(0, len(counttemp), 2)}\r\n",
        "    \r\n",
        "for i in counttemp:\r\n",
        "  if i=='12/1996':\r\n",
        "    break\r\n",
        "  elif(int(i[3:])>2017):\r\n",
        "    continue\r\n",
        "  counts.append(counttemp[i])\r\n",
        "counts = counts[4:-8]\r\n",
        "\r\n",
        "# Montando o dicionário inicial\r\n",
        "for i in range(0, len(datas)):\r\n",
        "  dicionario[datas[i]] = {  \r\n",
        "    \"count\":counts[i],\r\n",
        "    \"link\":links[i]\r\n",
        "  }\r\n",
        "\r\n",
        "print(dicionario)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IxZVAOyQIfG"
      },
      "source": [
        "# dicionário que guarda as informações de todos os registros coletados no site\r\n",
        "ovnis = {}\r\n",
        "chave = 0\r\n",
        "\r\n",
        "# percorre todo o dicionário\r\n",
        "for i in dicionario:\r\n",
        "  # lista que armazena as linhas das tabelas de cada página/link acessado\r\n",
        "  linhas = []\r\n",
        "  # acessa a página correspondente ao link em questão\r\n",
        "  pagina = link_suporte+dicionario[i]['link']\r\n",
        "  resposta = getPage(pagina)\r\n",
        "  soup = BeautifulSoup(resposta.read(), features=\"html.parser\")\r\n",
        "  linhas_tabela = soup.find_all(\"tr\")\r\n",
        "  \r\n",
        "  for l in linhas_tabela:\r\n",
        "    linhas.append(l.get_text().strip().split('\\n'))\r\n",
        "\r\n",
        "  # Armazenando as informações no dicionário principal (pré-csv)\r\n",
        "  for linha in linhas[1:]:\r\n",
        "    ovnis[str(chave)] = {\r\n",
        "        'data_hora': linha[0],\r\n",
        "        'cidade': linha[1],\r\n",
        "        'estado': linha[2],\r\n",
        "        'formato': linha[3],\r\n",
        "        'duracao': linha[4],\r\n",
        "        'resumo': linha[5],\r\n",
        "        'data_postagem': linha[6]\r\n",
        "    }\r\n",
        "    chave += 1\r\n",
        "\r\n",
        "  print(\"Ovnis: \", len(ovnis))\r\n",
        "\r\n",
        "  # Limitador para teste\r\n",
        "  '''if(len(ovnis)>=1367):\r\n",
        "    break'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-GAPiRXwvy_"
      },
      "source": [
        "# CELULA RESPONSAVEL POR CONVERTER O DICT PARA CSV\r\n",
        "\r\n",
        "csv_columns = ['ID', 'data_hora' , 'cidade', 'estado', 'formato', 'duracao', 'resumo', 'data_postagem']\r\n",
        "csvfile = 'OVNIS.csv'\r\n",
        "dict_data = [\r\n",
        "{'ID': i,\r\n",
        " 'data_hora': ovnis[i]['data_hora'],\r\n",
        " 'cidade': ovnis[i]['cidade'],\r\n",
        " 'estado': ovnis[i]['estado'],\r\n",
        " 'formato': ovnis[i]['formato'],\r\n",
        " 'duracao': ovnis[i]['duracao'],\r\n",
        " 'resumo': ovnis[i]['resumo'],\r\n",
        " 'data_postagem': ovnis[i]['data_postagem']} for i in ovnis\r\n",
        "]\r\n",
        "\r\n",
        "try:\r\n",
        "  with open(csvfile, 'w') as csvfile:\r\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\r\n",
        "    writer.writeheader()\r\n",
        "    for data in dict_data:\r\n",
        "      writer.writerow(data)\r\n",
        "except IOError:\r\n",
        "  print(\"I/O error\")\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}